{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 丢弃法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def dropout(X,drop_prob): # drop_prob表示丢弃概率\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, shape=X.shape) < keep_prob # 生成与X形状相同的矩阵，矩阵中的元素随机取0或1，概率为1-keep_prob和keep_prob\n",
    "    return mask * X / keep_prob  # 通过乘法运算将需要丢弃的元素设为0，通过除法运算将保留的元素重新缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=nd.arange(16).reshape((2,8))\n",
    "dropout(X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  4.  6.  0.  0.  0. 14.]\n",
       " [ 0. 18.  0.  0. 24. 26. 28.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "\n",
    "num_inputs,num_outputs,num_hiddens1,num_hiddens2 = 784,10,256,256\n",
    "\n",
    "w1=nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens1))\n",
    "b1=nd.zeros(num_hiddens1)\n",
    "w2=nd.random.normal(scale=0.01,shape=(num_hiddens1,num_hiddens2))\n",
    "b2=nd.zeros(num_hiddens2)\n",
    "w3=nd.random.normal(scale=0.01,shape=(num_hiddens2,num_outputs))\n",
    "b3=nd.zeros(num_outputs)\n",
    "\n",
    "params=[w1,b1,w2,b2,w3,b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "drop_prob1,drop_prob2 = 0.2, 0.5 # 设置丢弃概率，建议把靠近输入层的丢弃概率设置得稍小一些\n",
    "\n",
    "def net(X):\n",
    "    X=X.reshape((-1,num_inputs))\n",
    "    H1=(nd.dot(X,w1)+b1).relu()\n",
    "    if (autograd.is_training()):\n",
    "        # 在训练模型时，随机丢弃一部分数据\n",
    "        H1=dropout(H1,drop_prob1)\n",
    "    H2=(nd.dot(H1,w2)+b2).relu()\n",
    "    if (autograd.is_training()):\n",
    "        # 在训练模型时，随机丢弃一部分数据\n",
    "        H2=dropout(H2,drop_prob2)\n",
    "    return nd.dot(H2,w3)+b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1942, train acc 0.532, test acc 0.774\n",
      "epoch 2, loss 0.5952, train acc 0.779, test acc 0.829\n",
      "epoch 3, loss 0.4972, train acc 0.821, test acc 0.852\n",
      "epoch 4, loss 0.4540, train acc 0.835, test acc 0.853\n",
      "epoch 5, loss 0.4233, train acc 0.846, test acc 0.859\n",
      "epoch 6, loss 0.4015, train acc 0.855, test acc 0.872\n",
      "epoch 7, loss 0.3896, train acc 0.858, test acc 0.870\n",
      "epoch 8, loss 0.3704, train acc 0.865, test acc 0.878\n",
      "epoch 9, loss 0.3617, train acc 0.867, test acc 0.875\n",
      "epoch 10, loss 0.3511, train acc 0.871, test acc 0.877\n"
     ]
    }
   ],
   "source": [
    "# 训练和测试模型\n",
    "\n",
    "num_epochs,lr,batch_size=10,0.5,256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dropout(drop_prob1)) # 在第一个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dropout(drop_prob2))  # 在第二个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1899, train acc 0.540, test acc 0.748\n",
      "epoch 2, loss 0.5856, train acc 0.781, test acc 0.818\n",
      "epoch 3, loss 0.5252, train acc 0.811, test acc 0.852\n",
      "epoch 4, loss 0.4528, train acc 0.835, test acc 0.857\n",
      "epoch 5, loss 0.4252, train acc 0.845, test acc 0.849\n",
      "epoch 6, loss 0.4005, train acc 0.854, test acc 0.869\n",
      "epoch 7, loss 0.3860, train acc 0.859, test acc 0.872\n",
      "epoch 8, loss 0.3738, train acc 0.865, test acc 0.865\n",
      "epoch 9, loss 0.3624, train acc 0.868, test acc 0.873\n",
      "epoch 10, loss 0.3522, train acc 0.872, test acc 0.876\n"
     ]
    }
   ],
   "source": [
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以使用丢弃法应对过拟合，丢弃法只在训练模型是使用，在测试模型时不会使用丢弃法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2351, train acc 0.522, test acc 0.733\n",
      "epoch 2, loss 0.6200, train acc 0.766, test acc 0.828\n",
      "epoch 3, loss 0.5253, train acc 0.807, test acc 0.852\n",
      "epoch 4, loss 0.4758, train acc 0.826, test acc 0.851\n",
      "epoch 5, loss 0.4491, train acc 0.835, test acc 0.853\n",
      "epoch 6, loss 0.4348, train acc 0.840, test acc 0.860\n",
      "epoch 7, loss 0.4184, train acc 0.847, test acc 0.866\n",
      "epoch 8, loss 0.4038, train acc 0.851, test acc 0.864\n",
      "epoch 9, loss 0.3983, train acc 0.852, test acc 0.865\n",
      "epoch 10, loss 0.3865, train acc 0.859, test acc 0.867\n"
     ]
    }
   ],
   "source": [
    "# 将丢弃概率的超参数对调，靠近输入层的丢弃概率设置为0.5，靠近输出的丢弃概率设置为0.2\n",
    "\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dropout(drop_prob2)) # 在第一个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dropout(drop_prob1))  # 在第二个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3781, train acc 0.860, test acc 0.874\n",
      "epoch 2, loss 0.3732, train acc 0.862, test acc 0.876\n",
      "epoch 3, loss 0.3680, train acc 0.864, test acc 0.872\n",
      "epoch 4, loss 0.3607, train acc 0.867, test acc 0.871\n",
      "epoch 5, loss 0.3580, train acc 0.867, test acc 0.875\n",
      "epoch 6, loss 0.3500, train acc 0.871, test acc 0.878\n",
      "epoch 7, loss 0.3476, train acc 0.871, test acc 0.881\n",
      "epoch 8, loss 0.3417, train acc 0.872, test acc 0.883\n",
      "epoch 9, loss 0.3393, train acc 0.874, test acc 0.883\n",
      "epoch 10, loss 0.3376, train acc 0.876, test acc 0.883\n",
      "epoch 11, loss 0.3326, train acc 0.877, test acc 0.880\n",
      "epoch 12, loss 0.3295, train acc 0.877, test acc 0.883\n",
      "epoch 13, loss 0.3268, train acc 0.878, test acc 0.884\n",
      "epoch 14, loss 0.3219, train acc 0.880, test acc 0.887\n",
      "epoch 15, loss 0.3200, train acc 0.880, test acc 0.883\n",
      "epoch 16, loss 0.3140, train acc 0.883, test acc 0.888\n",
      "epoch 17, loss 0.3146, train acc 0.883, test acc 0.886\n",
      "epoch 18, loss 0.3111, train acc 0.884, test acc 0.888\n",
      "epoch 19, loss 0.3103, train acc 0.884, test acc 0.887\n",
      "epoch 20, loss 0.3076, train acc 0.885, test acc 0.890\n"
     ]
    }
   ],
   "source": [
    "# 增大迭代周期数，比较使用丢弃法和不使用丢弃法的结果\n",
    "# 使用丢弃法后，模型在训练集上的表现更好，但模型在测试集上的表现更差，说明使用丢弃法后模型过拟合更严重。\n",
    "num_epochs=20\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1309, train acc 0.561, test acc 0.778\n",
      "epoch 2, loss 0.6095, train acc 0.775, test acc 0.814\n",
      "epoch 3, loss 0.5072, train acc 0.818, test acc 0.849\n",
      "epoch 4, loss 0.4646, train acc 0.834, test acc 0.858\n",
      "epoch 5, loss 0.4437, train acc 0.841, test acc 0.860\n",
      "epoch 6, loss 0.4179, train acc 0.850, test acc 0.868\n",
      "epoch 7, loss 0.4018, train acc 0.856, test acc 0.861\n",
      "epoch 8, loss 0.3809, train acc 0.862, test acc 0.864\n",
      "epoch 9, loss 0.3713, train acc 0.866, test acc 0.876\n",
      "epoch 10, loss 0.3629, train acc 0.869, test acc 0.878\n"
     ]
    }
   ],
   "source": [
    "# 增大丢弃概率，比较使用丢弃法和不使用丢弃法的结果\n",
    "\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dropout(0.2)) # 在第一个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dropout(0.7))  # 在第二个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,10,batch_size,None,None,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24776\\2086381310.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_stale_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 更新参数，忽略陈旧梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch + 1}, Loss: {l.mean().asscalar()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SU_hole\\anaconda3\\envs\\mxnet_env\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2583\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2586\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2587\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SU_hole\\anaconda3\\envs\\mxnet_env\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2564\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2565\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2566\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   2567\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 同时使用丢弃法和权重衰减，比较使用丢弃法和不使用丢弃法的结果\n",
    "\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import nn, data as gdata, loss as gloss\n",
    "import d2l\n",
    "\n",
    "# 超参数设置\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "lr = 0.5\n",
    "drop_prob1 = 0.2  # 第1个丢弃层的丢弃率\n",
    "drop_prob2 = 0.7  # 第2个丢弃层的丢弃率\n",
    "\n",
    "# # 加载数据\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# # 定义损失函数\n",
    "# loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# 定义模型\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Dense(256, activation='relu'),\n",
    "    nn.Dropout(drop_prob2),  # 第1个丢弃层\n",
    "    nn.Dense(256, activation='relu'),\n",
    "    nn.Dropout(drop_prob1),  # 第2个丢弃层\n",
    "    nn.Dense(10)\n",
    ")\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "# 定义优化器\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'wd': 3})\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)  # 计算损失\n",
    "        l.backward()  # 反向传播\n",
    "        trainer.step(batch_size, ignore_stale_grad=True)  # 更新参数，忽略陈旧梯度\n",
    "    print(f'Epoch {epoch + 1}, Loss: {l.mean().asscalar()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.0519, train acc 0.597, test acc 0.791\n",
      "epoch 2, loss 0.5616, train acc 0.791, test acc 0.835\n",
      "epoch 3, loss 0.4764, train acc 0.826, test acc 0.852\n",
      "epoch 4, loss 0.4424, train acc 0.838, test acc 0.865\n",
      "epoch 5, loss 0.4160, train acc 0.849, test acc 0.869\n",
      "epoch 6, loss 0.3920, train acc 0.857, test acc 0.864\n",
      "epoch 7, loss 0.3755, train acc 0.863, test acc 0.877\n",
      "epoch 8, loss 0.3634, train acc 0.867, test acc 0.872\n",
      "epoch 9, loss 0.3485, train acc 0.873, test acc 0.878\n",
      "epoch 10, loss 0.3420, train acc 0.874, test acc 0.874\n"
     ]
    }
   ],
   "source": [
    "# 增加隐藏层的神经元数量，比较使用丢弃法和不使用丢弃法的结果\n",
    "\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(512,activation='relu'))\n",
    "net.add(nn.Dropout(drop_prob1)) # 在第一个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(512,activation='relu'))\n",
    "net.add(nn.Dropout(0.7))  # 在第二个全连接层之后添加丢弃层\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,10,batch_size,None,None,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
